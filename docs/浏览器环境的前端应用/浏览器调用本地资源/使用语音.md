# 使用语音

在`developer.mozilla.org`中规定了一种能够让浏览器处理语音数据的规范[`web speech`](https://developer.mozilla.org/zh-CN/docs/Web/API/Web_Speech_API),这还是个实验中的规范,目前应该只有chrome完全支持.

`web speech`支持两种功能:

1. 语音识别

     通过`SpeechRecognition`接口进行访问,它提供了识别从音频输入(通常是设备默认的语音识别服务)中识别语音情景的能力.一般来说,你将使用该接口的构造函数来构造一个新的`SpeechRecognition`对象,该对象包含了一些列有效的对象处理函数来检测识别设备麦克风中的语音输入.`SpeechGrammar`接口则表示了你应用中想要识别的特定文法.文法则通过`JSpeech Grammar Format`(JSGF.)来定义.


2. 语音合成

    通过`SpeechSynthesis`接口进行访问,它提供了文字到语音(TTS)的能力,这使得程序能够读出它们的文字内容(通常使用设备默认的语音合成器).不同的声音类类型通过`SpeechSynthesisVoice`对象进行表示,不同部分的文字则由`SpeechSynthesisUtterance`对象来表示.你可以将它们传递给`SpeechSynthesis.speak()` 方法来产生语音.

本节代码在[C2-S4](https://github.com/TutorialForJavascript/frontend-basic/tree/master/code/C2/S4)

## 语音识别

本段代码在[C2-S4-P1](https://github.com/TutorialForJavascript/frontend-basic/tree/master/code/C2/S4/P1).

这个例子我们调用[语音识别接口](https://developer.mozilla.org/zh-CN/docs/Web/API/Web_Speech_API#Browser_compatibility),通过读出`red`,`blue`等颜色来改变页面的背景色.

+ index.html
```html
<body>
    <main>
        <h1>说出颜色,用英文</h1>
        <p class="hints"></p>
        <div>
            <p class="output"><em>...判断的句子</em></p>
        </div>
    </main>
</body>
```

+ index.js
  
```js
const SpeechRecognition =  webkitSpeechRecognition || SpeechRecognition
const SpeechGrammarList =  webkitSpeechGrammarList || SpeechGrammarList
const SpeechRecognitionEvent = webkitSpeechRecognitionEvent || SpeechRecognitionEvent

const colors = [ 'aqua' , 'azure' , 'beige', 'bisque', 'black', 'blue', 'brown', 'chocolate', 'coral', 'crimson', 'cyan', 'fuchsia', 'ghostwhite', 'gold', 'goldenrod', 'gray', 'green', 'indigo', 'ivory', 'khaki', 'lavender', 'lime', 'linen', 'magenta', 'maroon', 'moccasin', 'navy', 'olive', 'orange', 'orchid', 'peru', 'pink', 'plum', 'purple', 'red', 'salmon', 'sienna', 'silver', 'snow', 'tan', 'teal', 'thistle', 'tomato', 'turquoise', 'violet', 'white', 'yellow'];
const grammar = '#JSGF V1.0; grammar colors; public <color> = ' + colors.join(' | ') + ' ;'
class SpeechRecongnitionApp{
    constructor() {
        // 初始化语音识别接口
        this.recognition = new SpeechRecognition()
        this.speechRecognitionList = new SpeechGrammarList()
        this.speechRecognitionList.addFromString(grammar, 1)
        this.recognition.grammars = this.speechRecognitionList
        this.recognition.lang = 'en-US'
        this.recognition.interimResults = false
        this.recognition.maxAlternatives = 1

        //控件
        this.diagnostic = document.querySelector('.output')
        this.bg = document.querySelector('html')
        this.hints = document.querySelector('.hints')
        this.bindEvent()
    }

    bindEvent(){
        document.body.onclick = ()=>this.startRecongnition()
        this.recognition.onresult = (event)=>this.onResult(event)
        this.recognition.onspeechend = ()=>this.onSpeeched()
        this.recognition.onnomatch = (event)=>this.onNoMatch()
        this.recognition.onerror = (event)=>this.onError(event)
    }

    htmlRender(){
        let colorHTML= ''
        colors.forEach(function(v, i, a){
        console.log(v, i)
        colorHTML += '<span style="background-color:' + v + ';"> ' + v + ' </span>'
        })
        this.hints.innerHTML = 'Tap/click then say a color to change the background color of the app. Try '+ colorHTML + '.'
    }
    startRecongnition(){
        this.recognition.start()
        console.log('Ready to receive a color command.')
    }
    onResult(event){
        let last = event.results.length - 1
        let color = event.results[last][0].transcript
        this.diagnostic.textContent = 'Result received: ' + color + '.'
        this.bg.style.backgroundColor = color
        console.log('Confidence: ' + event.results[0][0].confidence)
    }
    onSpeeched(){
        this.recognition.stop()
    }
    onNoMatch(){
        this.diagnostic.textContent = "I didn't recognise that color."
    }
    onError(event){
        this.diagnostic.textContent = 'Error occurred in recognition: ' + event.error
    }
}

function main() {
    let app = new SpeechRecongnitionApp()
    app.htmlRender()
}

main()
```

这个例子来自[Run recognition demo live](https://mdn.github.io/web-speech-api/speech-color-changer/),在<https://mdn.github.io/web-speech-api/>下还有几个有意思的语音识别例子,感兴趣可以看看.

### 语音识别接口的用法

大致分为几段:

+ 初始化

    我们需要初始化创建`SpeechRecognition`和`SpeechGrammarList`两个类的实例.

`SpeechGrammarList`是语音识别中使用的匹配规则列表,其中的对象都是`SpeechGrammar`对象,可以使用[JSGF](https://www.w3.org/TR/jsgf/)规则定义的字符串来设置规则,然后调用`.addFromString(grammar, 1)`这个方法来将规则放入`SpeechGrammarList`的对象列表中.每一个`SpeechGrammar`对象都可以设置属性`weight`来修改权重.

`SpeechRecognition`是语音识别的对象,可以设置属性
    
属性|说明
---|---
`lang:string`|语言
`interimResults:bool=false`|是否返回临时结果
`continuous:bool:false`|是否每个识别返回连续结果
`maxAlternatives:int=1`|最大被选方案数
`serviceURI:str`|默认为客户端默认的语音识别服务器可以设置.
`grammars:SpeechGrammarList`|设定语法规则列表

+ 定义语音识别的事件句柄的回调函数

`SpeechRecognition`的事件句柄有:

事件句柄|说明
---|---
`audiostart`|用户开始捕获音频时触发
`audioend`|用户完成捕获音频时触发
`start`|当语音识别服务已经开始收听传入音频时触发.
`end`|语音识别服务断开连接时触发.
`error`|发生语音识别错误时触发.
`nomatch`|当语音识别服务返回没有明显识别的最终结果时触发,这可能涉及某种程度的识别.说明其不满足或超过一个置信度阈值.
`result`|语音识别服务返回结果时触发.即单词或短语已被正确识别,并且已将其传回应用程序.
`soundstart`|在检测到任何声音,可识别的语音时触发.
`soundend`|当没有任何可辨认的声音时被触发.
`speechstart`|当识别出有语音时触发
`speechend`|当识别出语音结束时触发

我们可以根据需要为事件添加回调函数,最常用的有:

+ `start`
+ `speechend`
+ `error`
+ `nomatch`
+ `result`

+ 调用方法启动或者关停语音识别

方法|说明
---|---
`abort()`|停止语音识别服务和收听传入音频,并且不会尝试返回结果
`start()`|启动语音识别服务,听取传入的音频.
`stop()`|停止语音识别服务以收听传入音频,并尝试使用到目前为止捕获的音频返回结果.

通常我们需要一个外部的条件触发`start`方法,但`stop`可以是外部触发,也可以是放在语音识别的事件中,比如发现没有声音了就退出这样

## 语音合成

本段代码在[C2-S4-P2](https://github.com/TutorialForJavascript/frontend-basic/tree/master/code/C2/S4/P2).

不像语音识别接口,`SpeechSynthesis`这个接口多数浏览器都已经支持了.

这个例子我们制作一个表单,这个表单有一个文本框输入,一个下拉选单用于选语音,两个和一个提交按钮,点击提交或切换语音后我们使用语音合成读出选择的内容.

语音和成使用的是`window.speechSynthesis`接口其步骤是:


1. 使用`window.speechSynthesis.getVoices()`可以获取到音源列表
2. 创建`SpeechSynthesisUtterance(text:string)`的实例来构造一个发音对象
  + 设置发音对象的属性`voice`来定义一个音源(`window.speechSynthesis.getVoices()`这个中的一个元素)
  + 设置发音对象的属性`pitch`来定义发音对象的语调(float)
  + 设置发音对象的属性`rate`来定义语速(float)
3. 使用`window.speechSynthesis.speak(utter:SpeechSynthesisUtterance)`读出内容.